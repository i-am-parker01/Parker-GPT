# Parker-GPT
Mini GPT-2 Text Generator  A lightweight Hugging Faceâ€™s GPT-2 model to generate creative text completions from prompts.

## ðŸš€ Mini GPT-2 Text Generator

This project is a  Hugging Faceâ€™s GPT-2 model to generate text from user prompts. Itâ€™s a simple yet powerful way to explore transformer-based language models without needing heavy infrastructure or paid APIs.
## ðŸ”¹ Features:

## Text generation using GPT-2 (117M parameters).
## REST API with /generate endpoint.
## Adjustable max_length, temperature, and top_k for creative control.
## Interactive Swagger UI (/docs) for testing.
## Easy deployment locally, in Colab (with ngrok), or on cloud services (Render, Railway, Heroku).

## ðŸ”¹ Tech Stack:

Python 3.10+
FastAPI (Backend framework)
Hugging Face Transformers (Pretrained GPT-2)
PyTorch (Model inference)
Uvicorn (ASGI server)
